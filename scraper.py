import requests
import json
import os
from datetime import datetime
import time

# ==========================================
# 1. é…ç½®
# ==========================================
api_key = os.environ.get("RAPIDAPI_KEY")
# api_key = "ä½ çš„_TEST_KEY" 

if not api_key:
    print("âŒ Error: RAPIDAPI_KEY is missing.")
    exit(1)

url = "https://jsearch.p.rapidapi.com/search"

headers = {
    "X-RapidAPI-Key": api_key,
    "X-RapidAPI-Host": "jsearch.p.rapidapi.com"
}

# ==========================================
# 2. å…³é”®è¯åˆ†ç»„ (å®Œå…¨ç…§æ¬ä½ çš„ LinkedIn æœç´¢è¯)
# ==========================================

# ä¸ºäº†é˜²æ­¢ API æ¶ˆåŒ–ä¸è‰¯ï¼Œæˆ‘ä»¬å°†ä½ çš„é•¿åˆ—è¡¨æ‹†åˆ†ä¸º 3 ç»„
queries = [
    # ç»„ 1: æ ¸å¿ƒåˆ†æ
    '("People Analyst" OR "HR Data Analyst" OR "People Data Analyst" OR "Workforce Analytics" OR "Workforce Planning Analyst")',
    
    # ç»„ 2: ç³»ç»Ÿä¸æŠ€æœ¯
    '("HRIS Analyst" OR "HR Systems Analyst" OR "HR Tech Analyst" OR "Workday Analyst" OR "People Operations Analyst")',
    
    # ç»„ 3: è–ªé…¬ä¸ä½“éªŒ
    '("Compensation Analyst" OR "Total Rewards Analyst" OR "Talent Analytics" OR "Talent Insights" OR "Recruiting Data Analyst" OR "Employee Experience Analyst")'
]

# ==========================================
# 3. æ‰§è¡ŒæŠ“å–
# ==========================================

all_clean_jobs = []
seen_job_ids = set() 

print(f"ğŸš€ Starting California-specific scrape...")

for q in queries:
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šåœ°ç‚¹æ”¹ä¸º California, USA
    query_string = f"{q} in California, USA"
    
    params = {
        "query": query_string,
        "page": "1",
        "num_pages": "5",       # æ¯ä¸ªç»„æŠ“5é¡µ
        "date_posted": "3days", # ä¾ç„¶å»ºè®®ç”¨ 3daysï¼Œå› ä¸º API çš„æ—¶æ•ˆæ€§æ¯” LinkedIn ç¨å¾®æ»åä¸€ç‚¹ç‚¹
        "employment_types": "fulltime"
    }

    try:
        print(f"   ğŸ” Searching in CA: {q[:30]}...")
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        raw_jobs = data.get('data', [])
        print(f"      ğŸ“¦ Found {len(raw_jobs)} raw jobs.")
        
        # åƒåœ¾è¯é»‘åå•
        exclude_keywords = [
            "recruiter", "talent acquisition partner", "coordinator", "assistant", 
            "intern", "sales", "manager of", "head of", "director"
        ]

        for job in raw_jobs:
            title = job.get("job_title", "").lower()
            
            # 1. åƒåœ¾è¯è¿‡æ»¤
            if any(keyword in title for keyword in exclude_keywords):
                continue 
                
            # 2. ç”Ÿæˆ ID
            job_id = job.get("job_id") or job.get("job_apply_link")
            
            # 3. å»é‡
            if job_id and job_id not in seen_job_ids:
                seen_job_ids.add(job_id)
                
                all_clean_jobs.append({
                    "job_id": job_id,
                    "job_title": job.get("job_title"),
                    "employer_name": job.get("employer_name"),
                    "employer_logo": job.get("employer_logo"),
                    "job_city": job.get("job_city"),
                    "job_state": job.get("job_state"),
                    "job_country": job.get("job_country"),
                    "job_apply_link": job.get("job_apply_link"),
                    "job_posted_at_datetime_utc": job.get("job_posted_at_datetime_utc")
                })
        
        time.sleep(1)

    except Exception as e:
        print(f"      âš ï¸ Error fetching query: {e}")
        continue

# ==========================================
# 4. ä¿å­˜
# ==========================================

print(f"ğŸ‰ Total unique CA jobs: {len(all_clean_jobs)}")

final_data = {
    "last_updated": datetime.utcnow().isoformat(),
    "total_jobs": len(all_clean_jobs),
    "jobs": all_clean_jobs
}

os.makedirs('public', exist_ok=True)

with open('public/jobs.json', 'w', encoding='utf-8') as f:
    json.dump(final_data, f, ensure_ascii=False, indent=2)

print(f"âœ… Saved to public/jobs.json")