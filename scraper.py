import requests
import json
import os
from datetime import datetime
import time

# ==========================================
# 1. é…ç½® (Configuration)
# ==========================================
api_key = os.environ.get("RAPIDAPI_KEY")

# ğŸš¨ æœ¬åœ°æµ‹è¯•æ—¶å–æ¶ˆæ³¨é‡Šä¸‹é¢è¿™è¡Œå¡«å…¥ Keyï¼Œæäº¤ä»£ç å‰è®°å¾—æ³¨é‡Šæ‰ï¼
# api_key = "ä½ çš„_RAPIDAPI_KEY"

if not api_key:
    print("âŒ Error: RAPIDAPI_KEY is missing.")
    exit(1)

url = "https://jsearch.p.rapidapi.com/search"

headers = {
    "X-RapidAPI-Key": api_key,
    "X-RapidAPI-Host": "jsearch.p.rapidapi.com"
}

# ==========================================
# 2. æœç´¢æˆ˜é˜Ÿ (å…³é”®è¯åˆ†ç»„ç­–ç•¥)
# ==========================================
# ä½ æä¾›çš„ 16 ä¸ªå…³é”®è¯å¤ªé•¿äº†ï¼Œä¸€æ¬¡æœä¸å®Œã€‚
# æˆ‘ä»¬æŠŠå®ƒä»¬æŒ‰â€œèŒèƒ½â€æ‹†åˆ†æˆ 3 ç»„ï¼Œç¡®ä¿æ¯ä¸ªèŒä½éƒ½èƒ½è¢«æŠ“åˆ°ã€‚

queries = [
    # æˆ˜é˜Ÿ A: æ•°æ®ä¸è§„åˆ’ (Data & Planning)
    '("People Analyst" OR "HR Data Analyst" OR "People Data Analyst" OR "Workforce Analytics" OR "Workforce Planning Analyst")',
    
    # æˆ˜é˜Ÿ B: ç³»ç»Ÿä¸æŠ€æœ¯ (Systems & Tech)
    '("HRIS Analyst" OR "HR Systems Analyst" OR "HR Tech Analyst" OR "Workday Analyst" OR "People Operations Analyst")',
    
    # æˆ˜é˜Ÿ C: è–ªé…¬ã€äººæ‰ä¸ä½“éªŒ (Comp, Talent & Experience)
    '("Compensation Analyst" OR "Total Rewards Analyst" OR "Talent Analytics" OR "Talent Insights" OR "Recruiting Data Analyst" OR "Employee Experience Analyst")'
]

# ==========================================
# 3. æ‰§è¡ŒæŠ“å– (Execution)
# ==========================================

all_clean_jobs = []
seen_job_ids = set() 

print(f"ğŸš€ Starting scrape for: California, Past 24 Hours...")

for q in queries:
    # ğŸ“ æ ¸å¿ƒä¿®æ”¹ï¼šç²¾å‡†é”å®šåŠ å·
    query_string = f"{q} in California, USA"
    
    params = {
        "query": query_string,
        "page": "1",
        "num_pages": "10",       # æ¯ä¸ªæˆ˜é˜ŸæŠ“ 10 é¡µ (ä¿è¯è¦†ç›–é‡)
        "date_posted": "today", # ğŸ•’ æ ¸å¿ƒä¿®æ”¹ï¼šåªæŠ“ä»Šå¤© (Past 24h)
        "employment_types": "fulltime"
    }

    try:
        print(f"   ğŸ” Searching: {q[:40]}...")
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        raw_jobs = data.get('data', [])
        print(f"      ğŸ“¦ Found {len(raw_jobs)} raw jobs.")
        
        # åƒåœ¾è¯é»‘åå• (æ’é™¤é Analyst èŒä½)
        exclude_keywords = [
            "recruiter", "talent acquisition partner", "coordinator", "assistant", 
            "intern", "sales", "account executive", "business development", 
            "manager of", "director", "vp", "head of"
        ]

        for job in raw_jobs:
            title = job.get("job_title", "").lower()
            
            # 1. åƒåœ¾è¯è¿‡æ»¤
            if any(keyword in title for keyword in exclude_keywords):
                continue 
                
            # 2. ç”Ÿæˆ ID
            job_id = job.get("job_id") or job.get("job_apply_link")
            
            # 3. å»é‡ (é˜²æ­¢æˆ˜é˜Ÿ A å’Œ B æŠ“åˆ°åŒä¸€ä¸ª)
            if job_id and job_id not in seen_job_ids:
                seen_job_ids.add(job_id)
                
                all_clean_jobs.append({
                    "job_id": job_id,
                    "job_title": job.get("job_title"),
                    "employer_name": job.get("employer_name"),
                    "employer_logo": job.get("employer_logo"),
                    "job_city": job.get("job_city"),
                    "job_state": job.get("job_state"),
                    "job_country": job.get("job_country"),
                    "job_apply_link": job.get("job_apply_link"),
                    "job_posted_at_datetime_utc": job.get("job_posted_at_datetime_utc")
                })
        
        # ä¼‘æ¯ 1 ç§’ï¼Œå¯¹ API æ¸©æŸ”ä¸€ç‚¹
        time.sleep(1)

    except Exception as e:
        print(f"      âš ï¸ Error fetching query: {e}")
        continue

# ==========================================
# 4. ä¿å­˜æ•°æ® (Save)
# ==========================================

print(f"ğŸ‰ Total unique CA jobs found: {len(all_clean_jobs)}")

final_data = {
    "last_updated": datetime.utcnow().isoformat(),
    "total_jobs": len(all_clean_jobs),
    "jobs": all_clean_jobs
}

os.makedirs('public', exist_ok=True)

with open('public/jobs.json', 'w', encoding='utf-8') as f:
    json.dump(final_data, f, ensure_ascii=False, indent=2)

print(f"âœ… Saved to public/jobs.json")